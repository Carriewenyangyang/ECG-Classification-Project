{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uu-sml/wasp-assigninmen-af-classification/blob/main/assignment_ecg_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z00pmDU9qUai"
      },
      "source": [
        "# WASP Course: Artificial Intelligence and Machine Learning\n",
        "\n",
        "Lecturer: Dave Zachariah\n",
        "\n",
        "Assignment responsible: Jingwei Hu, Tianru Zhang, David Vävinggren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGfRFsjdWGyp"
      },
      "source": [
        "# Student and Group Information\n",
        "\n",
        "Fill this out for the submission of the assignment (you submit this notebook with your solution)\n",
        "\n",
        "- **Student names:** <font color='red'>Yangyang Wen</font>\n",
        "\n",
        "\n",
        "- **Team ID:** <font color='red'>YangyangWen_Group25</font>\n",
        "\n",
        "\n",
        "Make sure that the team id is the same as the one with which you submit your model predictions (see coding task 7) such that we can check your performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_X8-WK_Htgr"
      },
      "source": [
        "---\n",
        "# Module 3 - Assignment Overview: ECG classification\n",
        "\n",
        "The [electrocardiogram (ECG)](https://www.mayoclinic.org/tests-procedures/ekg/about/pac-20384983) records the electrical signals in the heart. It is a common  test used to quickly detect heart problems and to monitor the heart's health.\n",
        "In this assignment you will implement and evaluate a model to classify whether the person has [atrial fibrillation (AF)](https://www.mayoclinic.org/diseases-conditions/atrial-fibrillation/symptoms-causes/syc-20350624.) or not based on measurements from the ECG exam.\n",
        "\n",
        "\n",
        "**Submission:** You submit the deliverables (see below) at https://canvas.kth.se/courses/54581/assignments\n",
        "\n",
        "**Due Date:** August 22, 2025.\n",
        "\n",
        "---\n",
        "## Basic Tasks\n",
        "Your task is to implement a classification model, train this model on training data, and evaluate its performance on validation data. We provide skeleton code for the implementation of a simple convolution neural network model.\n",
        "\n",
        "The steps required to implement this model are presented as numbered tasks below. In total there are seven (7) coding tasks and five (5) explanation tasks.\n",
        "\n",
        "## Competitive setting\n",
        "\n",
        "You have to compute the predictions for the test data (you do not have the labels for it) and submit your predictions to be evaluated to a leaderboard. These predictions will be scored and your submission will be ranked according to the F1 score and compared with your colleagues. In the end a winning team will be determined.\n",
        "\n",
        "### Deliverables\n",
        "There are two deliverables:\n",
        "1. You have to submit this Jupyter notebook on the course web-page (Canvas) together with your code and explanations (where asked for it) that describe your implementation and your experimental results. The notebook should run as a standalone in google colab.\n",
        "2. You have to have at least **three (3)** submissions (for instructions on how to submit, see coding task 7) where you try to improve the model architecture, the training procedure or the problem formulation. In the submission of this notebook you have to provide a short explanation of what changed between each submission and justify why you decided to make these changes.\n",
        "\n",
        "### Grading\n",
        "To pass the assignment, you must submit a complete and working implementation of a model and a well-motivated description and evaluation of it. Your model should reach an Area under the ROC curve (AUROC) on the test data of at least 0.97 and an Average Precision (AP) score of 0.95. Note that the leaderboard to is sorted by F1 score and not AUROC, hence you would want to balance all three metrics.\n",
        "\n",
        "### GPU Acceleration\n",
        "To be able to use the GPUs provided by colab in order to speed up your computations, you want to check that the `Hardware accelerator` is set to `GPU` under `Runtime > change runtime type`. Note that notebooks run by connecting to virtual machines that have maximum lifetimes that can be as much as 12 hours. Notebooks will also disconnect from VMs when left idle for too long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEaQdHAdWGyp",
        "outputId": "91154ed4-6500-4c4b-c7bb-61bf59dc5475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "requirements.txt already exits. Using cached. Delete it manually to recieve it again!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# helper function\n",
        "def exists(path):\n",
        "    val = os.path.exists(path)\n",
        "    if val:\n",
        "        print(f'{path} already exits. Using cached. Delete it manually to recieve it again!')\n",
        "    return val\n",
        "\n",
        "# clone requirements.txt if not yet available\n",
        "if not exists('requirements.txt'):\n",
        "    !git clone https://gist.github.com/dgedon/8a7b91714568dc35d0527233e9ceada4.git req\n",
        "    !mv req/requirements.txt .\n",
        "    !yes | rm -r req"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ_o0wKcm7WM",
        "outputId": "7ad5c89d-0668-45d0-903d-a62e0b13e5fe"
      },
      "outputs": [],
      "source": [
        "# Install packages (python>=3.9 is required)\n",
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JkjZfvuKHd6q",
        "outputId": "38fd4a7c-b5b5-474c-b8c2-6c630fce5308"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTfxCz1YIpGP"
      },
      "source": [
        "---\n",
        "## The data set\n",
        "\n",
        "The dataset is a subset of the [*CODE dataset*](https://scilifelab.figshare.com/articles/dataset/CODE_dataset/15169716): an anotated database of ECGs. The ECG exams were recorded in Brazil by the Telehealth Network of the state Minas Gerais between 2010 and 2016. The dataset and its usage for the development of deep learning methods was described in [\"Automatic diagnosis of the 12-lead ECG using a deep neural network\"](https://www.nature.com/articles/s41467-020-15432-4).\n",
        "The full dataset is available for research upon request.\n",
        "\n",
        "\n",
        "For the training dataset you have labels.\n",
        "For the test dataset you only have the ECG exams but no labels. Evaluation is done by submitting to the leaderboard.\n",
        "\n",
        "Download the dataset from the given dropbox link and unzip the folder containing the files. The downloaded files are in WFDB format (see [here](https://www.physionet.org/content/wfdb-python/3.4.1/) for details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PeNTb95FM6R1"
      },
      "outputs": [],
      "source": [
        "# 1. Download dataset\n",
        "if not exists('codesubset.tar.gz'):\n",
        "    !wget https://www.dropbox.com/s/9zkqa5y5jqakdil/codesubset.tar.gz?dl=0 -O codesubset.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rQR2EI49OVP0"
      },
      "outputs": [],
      "source": [
        "# 1. unzip the downloaded data set folder\n",
        "if not exists('codesubset'):\n",
        "    !tar -xf codesubset.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpZJEWhhWGyq"
      },
      "source": [
        "Note that the extraced folder 'codesubset' contains\n",
        "1. subfolders with the ECG exam traces. These have to be further preprocessed which we do in the next steps.\n",
        "2. a csv file which contain the labels and other features for the training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H8tsO-QABmw"
      },
      "source": [
        "\n",
        "### Preprocessing\n",
        "\n",
        "Run the cells below to  Clone the GitHub repository which we use for [data preprocessing](https://github.com/antonior92/ecg-preprocessing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7yz7VW7nQEO-"
      },
      "outputs": [],
      "source": [
        "# 2. clone the code files for data preprocessing\n",
        "if not exists('ecg-preprocessing'):\n",
        "    !git clone https://github.com/paulhausner/ecg-preprocessing.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlvD4bEtQUhc"
      },
      "source": [
        "Let us plot an ECG sample. We can plot ECGs using the `ecg_plot` library for example by using the following code snippet where `ecg_sample` is an array of size `(number of leads * sequence length)`. Now we can view an ECG before preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QOpX0vlEQVUU"
      },
      "outputs": [],
      "source": [
        "import ecg_plot\n",
        "runfile(\"ecg-preprocessing/read_ecg.py\")\n",
        "\n",
        "PATH_TO_WFDB = 'codesubset/train/TNMG100046'\n",
        "ecg_sample, sample_rate, _ = read_ecg(PATH_TO_WFDB)\n",
        "\n",
        "# ECG plot\n",
        "plt.figure()\n",
        "lead = ['I', 'II', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
        "ecg_plot.plot(ecg_sample, sample_rate=sample_rate, style='bw', row_height=8, lead_index=lead, columns=1, title='Sample ECG before pre-processing')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_icbSxgDowE"
      },
      "source": [
        "\n",
        "The preprocessing consist of:\n",
        "- resampling all ECG traces to the sample sampling period (400 Hz). Option: ``--new_freq 400``\n",
        "- zero padding if necessary such that all ECG have the same number of samples (4096). Option: ``--new_len 4096``.\n",
        "- removing trends in the ECG signal. Option: ``--remove_baseline``\n",
        "- remove possible power line noise. Option: ``--powerline 60``\n",
        "\n",
        "You can run the script bellow to plot the same ECG after the preprocessing.  The script also use the  `ecg_plot` library (as you did above).  You can try also with different command line options to see how the preprocessing affects the signal that will be used by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lDQfCECcDoGN"
      },
      "outputs": [],
      "source": [
        "%run ecg-preprocessing/plot_from_ecg.py codesubset/train/TNMG100046 --new_freq 400 --new_len 4096 --remove_baseline --powerline 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Kh8RkYD8bE"
      },
      "source": [
        "\n",
        "Next we perform the preprocessing in all exams and convert them into one single h5 file (see [here](https://www.h5py.org/#:~:text=The%20h5py%20package%20is%20a,they%20were%20real%20NumPy%20arrays.) for details about the format). The resulting h5 files contains the traces as arrays with the shape `(number of traces * sequence length * number of leads)` where sequence length is 4096 and number of leads is 8.\n",
        "The files `train.h5` and `test.h5` will be saved inside the folder `codesubset/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cyS0WXTzQU5c"
      },
      "outputs": [],
      "source": [
        "# 3. Generate train\n",
        "if not exists('codesubset/train.h5'):\n",
        "    !python ecg-preprocessing/generate_h5.py --new_freq 400 --new_len 4096 --remove_baseline --powerline 60 codesubset/train/RECORDS.txt codesubset/train.h5\n",
        "# 3. Generate test\n",
        "if not exists('codesubset/test.h5'):\n",
        "    !python ecg-preprocessing/generate_h5.py --new_freq 400 --new_len 4096 --remove_baseline --powerline 60 codesubset/test/RECORDS.txt codesubset/test.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvelAs8NJExH"
      },
      "source": [
        "### Coding Task 1: Data Analysis\n",
        "\n",
        "Before starting to model you have to analyse the dataset. You can be creative in your way of *getting a feeling* for the data. What you have to do is:\n",
        "- plot an ECG after proprocessing saved in the hdf5 file. For this use the `ecg_plot()` example above and see below for how to access the preprocessed data in h5 format.\n",
        "\n",
        "Some further ideas to explore are:\n",
        "- check the balance of the data set,\n",
        "- evaluate the distribution of age and sex of the patients,\n",
        "- think about the performance that a best naive classifier would achieve, e.g. by random guessing or always predicting one class.\n",
        "\n",
        "<br />\n",
        "\n",
        "**How to access the data?**\n",
        "\n",
        "You can acces the data in the h5 file in the following way\n",
        "```\n",
        "import h5py\n",
        "\n",
        "PATH_TO_H5_FILE = 'codesubset/train.h5'\n",
        "f = h5py.File(PATH_TO_H5_FILE, 'r')\n",
        "data = f['tracings']\n",
        "```\n",
        "Then, `data[i]` is an numpy array of the $i$th ECG exam (including all time points and leads).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu4tpfc3STcD"
      },
      "source": [
        "### Explanation task 1: Data Analysis\n",
        "\n",
        "Please explain your main findings of the data analysis task in a few bullet points. Explain also what the preprocessing does and why it is necessary.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "**<font color='red'>Your explanation here:</font>**\n",
        "\n",
        "Main findings from data analysis:\n",
        "\n",
        "By comparing ECG plots before and after preprocessing, I found that (1) the waveform cycles in the traces become wider and stretched after preprocessing, reflecting the original sampling rates like 250 Hz being unified to 400 Hz; (2) zero-padding was applied at the beginning and end of all 8 leads to ensure the completeness and uniform length of the data.\n",
        "\n",
        "What preprocessing does:\n",
        "① Resampling: all ECG traces are resampled to 400 Hz, resulting in 4096 data points for a 10-second segment.\n",
        "② Zero padding: for signals shorter than 10 seconds, zeros are added at the start and end to reach 4096 points, satisfying the fixed input size required by neural networks.\n",
        "③ Baseline removal: trends and slow drifts in the ECG signal are removed, resulting in a more stable baseline that helps in extracting true cardiac features.\n",
        "④ Powerline noise removal: 60 Hz electrical interference is filtered out to reduce environmental noise and improve signal quality.\n",
        "\n",
        "Why preprocessing is necessary:\n",
        "① To ensure all traces have consistent sampling rates and lengths for model training.\n",
        "② To remove noise and baseline drift, improving signal quality and enabling the model to learn meaningful features.\n",
        "③ To reduce environmental interference, making model predictions more stable and accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ja8j1xOYJdqg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TASK: Insert your code here\n",
        "\"\"\"\n",
        "import h5py\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PATH_TO_H5_FILE = 'codesubset/train.h5'\n",
        "f = h5py.File(PATH_TO_H5_FILE, 'r')\n",
        "data = f['tracings']\n",
        "\n",
        "# Take 0th sample\n",
        "ecg_sample= data[0] # shape is (4096, 8), but ecg_plot needs (number_of_leads, sequence_length), here is one sample\n",
        "\n",
        "ecg_sample = ecg_sample.T\n",
        "\n",
        "plt.figure()\n",
        "lead = ['I', 'II', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
        "ecg_plot.plot(ecg_sample, sample_rate=400, style='bw', row_height=8, lead_index=lead, columns=1, title='Sample ECG after preprocessing')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XCbJ_xI7B2iz"
      },
      "outputs": [],
      "source": [
        "# 1. Calculate the proportion of each label (data balance)\n",
        "import pandas as pd\n",
        "\n",
        "# read label CSB\n",
        "df = pd.read_csv('codesubset/train.csv')\n",
        "#print(df)\n",
        "# Calculate the number of each category\n",
        "label_counts = df['AF'].value_counts()\n",
        "print(label_counts)\n",
        "# Caculate the ratio\n",
        "label_ratio = label_counts / len(df)\n",
        "print(label_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TbYSRI1cGMij"
      },
      "outputs": [],
      "source": [
        "# 2. Analyze age and gender distribution\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Age distribution histogram\n",
        "plt.hist(df['age'].dropna(), bins=30, color='skyblue')\n",
        "plt.title('Age distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Gender distribution (assuming the sex column is represented by 0 and 1)\n",
        "sex_counts = df['sex'].value_counts()\n",
        "sex_counts.index = ['Female', 'Male'] # Modify based on actual labels\n",
        "sex_counts.plot(kind='bar', color=['pink', 'lightblue'])\n",
        "plt.title('Sex distribution')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-oXc7mRiHkY-"
      },
      "outputs": [],
      "source": [
        "# 3. Simple \"naive baseline classifier\"\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# true labels\n",
        "y_true = df['AF'].values\n",
        "\n",
        "# baseline prediction: predict all as 0\n",
        "y_pred = [0]*len(y_true) # [1] is tried as well\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f'Baseline Accuracy: {acc:.4f}')\n",
        "print(f'Baseline F1 Score: {f1:.4f}')\n",
        "# This shows the model completely failed to predict positive examples such as \"atrial fibrillation\" (AF=1) (both recall and precision are 0), resulting in an F1 score of zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "keKS1r2MOZmW"
      },
      "outputs": [],
      "source": [
        "# 4. Comparison analysis before and after preprocessing\n",
        "\n",
        "import h5py\n",
        "import wfdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === 1. Read ECG samples before preprocessing (WFDB format) ===\n",
        "PATH_TO_WFDB = 'codesubset/train/TNMG100046'\n",
        "record = wfdb.rdrecord(PATH_TO_WFDB)\n",
        "ecg_before = record.p_signal.T  # (leads, length)\n",
        "fs_before = record.fs\n",
        "\n",
        "# === 2. Read ECG samples before preprocessing（h5 files）===\n",
        "PATH_TO_H5_FILE = 'codesubset/train.h5'\n",
        "f = h5py.File(PATH_TO_H5_FILE, 'r')\n",
        "ecg_after = f['tracings'][0].T  # (leads, length)\n",
        "\n",
        "fs_after = 400  # Specify preprocessing parameters --new_freq 400\n",
        "\n",
        "# === 3. Select a lead（e.g., lead 0）for comparison ===\n",
        "lead = 0\n",
        "signal_before = ecg_before[lead]\n",
        "signal_after = ecg_after[lead]\n",
        "\n",
        "# === 4. Waveform comparison ===\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(signal_before, label='Before preprocessing')\n",
        "plt.plot(signal_after, label='After preprocessing')\n",
        "plt.legend()\n",
        "plt.title(f\"Lead {lead} waveform comparison\")\n",
        "plt.show()\n",
        "\n",
        "# === 5. Difference waveform ===\n",
        "#plt.figure(figsize=(12, 4))\n",
        "#plt.plot(signal_before[:len(signal_after)] - signal_after, color='red')\n",
        "#plt.title(f\"Lead {lead} difference (before - after)\")\n",
        "#plt.show()\n",
        "\n",
        "# === 6. Spectrum comparison ===\n",
        "def plot_fft(signal, fs, label):\n",
        "    N = len(signal)\n",
        "    freqs = np.fft.rfftfreq(N, 1/fs)\n",
        "    fft_vals = np.abs(np.fft.rfft(signal))\n",
        "    plt.plot(freqs, fft_vals, label=label)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plot_fft(signal_before, fs_before, \"Before\")\n",
        "plot_fft(signal_after, fs_after, \"After\")\n",
        "plt.xlim(0, 100)  # Focus on the low-frequency band to easily identify the power frequency (60Hz) position\n",
        "plt.legend()\n",
        "plt.title(f\"Lead {lead} frequency spectrum comparison\")\n",
        "plt.xlabel(\"Frequency (Hz)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0-jgf-3qO-yH"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import resample\n",
        "\n",
        "signal_before_resampled = resample(signal_before, 4096)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(signal_before_resampled - signal_after, color='red')\n",
        "plt.title(f\"Lead {lead} difference (before - after)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO_E8qGUJ2Db"
      },
      "source": [
        "---\n",
        "## Model\n",
        "\n",
        "The model class consists of two methods:\n",
        "- `__init__(self, args)`: This methods initializes the class, e.g. by using `mymodel=ModelBaseline(args)`.\n",
        "- `forward(self,input_data)`: This method is called when we run `model_output=mymodel(input_data)`.\n",
        "\n",
        "The dimension of the input data is  `(batch size * sequence length * number of leads)`. Where **batch size** is a hyperparameter, **sequence length** is the number of ECG time samples (=4096) and **number of leads** (=8).\n",
        "\n",
        "The `ModelBaseline` (provided below) is a 2 layer model with one convolutional layers and one linear layer. Some explanations:\n",
        "- The conv layer downsamples the input traces from 4096 samples to 128 samples and increases the number of channels from 8 (=number of leads) to 32. Here we use a kernel size of 3.\n",
        "- The linear layer uses the flattened output from the conv and outputs one prediction. Since we have a binary problem, a single prediction is sufficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pONc-F25K-Z5"
      },
      "outputs": [],
      "source": [
        "class ModelBaseline(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(ModelBaseline, self).__init__()\n",
        "        self.kernel_size = 3\n",
        "\n",
        "        # conv layer\n",
        "        downsample = self._downsample(4096, 128)\n",
        "        self.conv1 = nn.Conv1d(in_channels=8,\n",
        "                               out_channels=32,\n",
        "                               kernel_size=self.kernel_size,\n",
        "                               stride=downsample,\n",
        "                               padding=self._padding(downsample),\n",
        "                               bias=False)\n",
        "\n",
        "        # linear layer\n",
        "        self.lin = nn.Linear(in_features=32*128,\n",
        "                             out_features=1)\n",
        "\n",
        "        # ReLU\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def _padding(self, downsample):\n",
        "        return max(0, int(np.floor((self.kernel_size - downsample + 1) / 2)))\n",
        "\n",
        "    def _downsample(self, seq_len_in, seq_len_out):\n",
        "        return int(seq_len_in // seq_len_out)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= x.transpose(2,1)\n",
        "\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x_flat= x.view(x.size(0), -1)\n",
        "        x = self.lin(x_flat)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q5oKHb1Ls87"
      },
      "source": [
        "### Coding Task 2: Define your model\n",
        "\n",
        "In the cell below you have to define your model. You can be inspired by the baseline model above but you can also define any other kind of neural network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1BHovxZZLvkd"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_leads=8):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=num_leads, out_channels=32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.branch1 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.branch2 = nn.Conv1d(32, 64, kernel_size=15, padding=7)\n",
        "        self.branch3 = nn.Conv1d(32, 64, kernel_size=25, padding=12)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(64*3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(64*3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.conv1(x)\n",
        "        x = torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vRvOiOE-RBED"
      },
      "outputs": [],
      "source": [
        "class LightCNN(nn.Module):\n",
        "    def __init__(self, num_leads=8):\n",
        "        super(LightCNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(num_leads, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2)  # Downsampling reducce the amount of computation required\n",
        "        )\n",
        "\n",
        "        self.branch1 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
        "        self.branch2 = nn.Conv1d(16, 32, kernel_size=7, padding=3)  # reduce convolution kernels\n",
        "        self.branch3 = nn.Conv1d(16, 32, kernel_size=11, padding=5)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(32*3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(32*3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.conv1(x)\n",
        "        x = torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kLwHklR2TUCE"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet1D(nn.Module):\n",
        "    def __init__(self, num_leads=8):\n",
        "        super(ResNet1D, self).__init__()\n",
        "        self.layer1 = ResidualBlock(num_leads, 16)\n",
        "        self.layer2 = ResidualBlock(16, 32, stride=2)\n",
        "        self.layer3 = ResidualBlock(32, 64, stride=2)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        #self.dropout = nn.Dropout(p=0.3) # My Design\n",
        "        self.fc = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        #x = self.dropout(x) # My Design\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BrnEzXGICiT5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# SE attention module(Squeeze-and-Excitation)\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, channels, time]\n",
        "        y = x.mean(-1)  # Global Avg Pool -> [batch, channels]\n",
        "        y = self.relu(self.fc1(y))\n",
        "        y = self.sigmoid(self.fc2(y))\n",
        "        y = y.unsqueeze(-1)  # [batch, channels, 1]\n",
        "        return x * y  # Channel weighting\n",
        "\n",
        "# Upgraded residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, kernel_size=5, dropout=0.2):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.se = SEBlock(out_channels)  # attention module\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.se(out)  # attention\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Upgraded ResNet1D\n",
        "class ResNet1D_Improved(nn.Module):\n",
        "    def __init__(self, num_leads=8, num_classes=1):\n",
        "        super(ResNet1D_Improved, self).__init__()\n",
        "        self.layer1 = ResidualBlock(num_leads, 32, kernel_size=7, dropout=0.1)  # bigger convolution kernels\n",
        "        self.layer2 = ResidualBlock(32, 64, stride=2, kernel_size=5, dropout=0.2)\n",
        "        self.layer3 = ResidualBlock(64, 128, stride=2, kernel_size=5, dropout=0.3)\n",
        "        self.layer4 = ResidualBlock(128, 128, stride=2, kernel_size=3, dropout=0.3)  # Add a new layer\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)  # [B, Leads, Time]\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0o3WWCTA604G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define BasicBlock（2-layer convolution + residual connection）\n",
        "class BasicBlock1D(nn.Module):\n",
        "    expansion = 1  # The bottleneck layer is generally 4, but here it is BasicBlock, so it is 1.\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, dropout=0.0):\n",
        "        super(BasicBlock1D, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        self.downsample = downsample  # Used to change the dimension matching residuals\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# The main structure of ResNet32\n",
        "class ResNet1D32(nn.Module):\n",
        "    def __init__(self, block=BasicBlock1D, layers=[5,5,5], num_classes=1, num_leads=8, dropout=0.2):\n",
        "        super(ResNet1D32, self).__init__()\n",
        "        self.in_channels = 16\n",
        "\n",
        "        # Input channel transformation layer\n",
        "        self.conv1 = nn.Conv1d(num_leads, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Three stages, each stage containing several BasicBlocks\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0], stride=1, dropout=dropout)\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2, dropout=dropout)  # downsample\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2, dropout=dropout)  # downsample\n",
        "\n",
        "        # Global average pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        # classification layer\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1, dropout=0.0):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(out_channels * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample, dropout=dropout))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels, dropout=dropout))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, leads, time]\n",
        "        x = x.transpose(1, 2)  # change to [batch, channels, length]\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x).squeeze(-1)  # [batch, channels]\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "k0pJ-3kEVTEp",
        "outputId": "50a4800c-e9c5-4d06-fd9a-93e91c678ba8"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, num_leads=8, hidden_size=64, num_layers=2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(num_leads, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, num_leads)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]  # last moment\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iOzDv49mVXHE"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, num_leads=8, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.input_proj = nn.Linear(num_leads, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ni22XUIxVaep"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, num_leads=8, hidden_size=64, num_layers=1):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(num_leads, hidden_size, num_layers=num_layers, batch_first=True, nonlinearity='tanh')\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7besXJ1Qjax"
      },
      "source": [
        "### Explanation Task 2: Final Model\n",
        "Please explain and motivate in short sentences or bullet points the choice of your final model.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "**<font color='red'>Your explanation here:</font>**\n",
        "\n",
        "I selected a CNN (Convolutional Neural Network) as the final model. According to the question requirements, I focused on improving the F1 score while also considering AUROC and Average Precision to balance precision and recall, ensuring the model performs well in distinguishing positive and negative samples. The reasons for choosing this model include:\n",
        "\n",
        "1️⃣ Multi-scale convolutions (kernel sizes = 3, 15, 25) designed in parallel, enabling the simultaneous capture of both local and long-term dependency features of ECG signals, which is suitable for the multi-frequency characteristics of electrocardiograms.\n",
        "2️⃣ Incorporation of BatchNorm and ReLU to enhance training stability and nonlinear representation capabilities.\n",
        "3️⃣ Use of global pooling and fully connected layers with moderate parameter counts to improve model generalization.\n",
        "4️⃣ Direct output of sigmoid probabilities for convenient threshold adjustment in downstream tasks.\n",
        "5️⃣ I also tested LSTM, RNN, and Transformer models on this dataset. I found that LSTM and RNN achieved lower accuracy than the CNN, making them less suitable for this scenario. The Transformer model was much slower to train and ran out of memory on this dataset.\n",
        "\n",
        "This approach is scalable to integrated learning across different architectures, facilitating future optimizations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeWVnhT1L72u"
      },
      "source": [
        "---\n",
        "## Train function\n",
        "\n",
        "The function `train(...)` is called to in every epoch to train the model. The function loads the training data, makes predictions, compares predictions with true labels in the loss function and adapting the model parameters using stochastic gradient descent.\n",
        "\n",
        "In the code cell below there is the basic structure to load data from the data loader and to log your loss. The arguments of the function are explained by the use in the `main(...)` function below.\n",
        "\n",
        "If you are unfamiliar with PyTorch training loops, then this official [tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) might help (especially section \"4. Train your Network\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHcflxJ1Wprw"
      },
      "source": [
        "### Coding Task 3: Fill training loop\n",
        "\n",
        "Fill the code cell below such that the model is training when `train(...)` is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QMLCx9Cahr7f"
      },
      "outputs": [],
      "source": [
        "def train_loop(epoch, dataloader, model, optimizer, loss_function, device):\n",
        "    # model to training mode (important to correctly handle dropout or batchnorm layers)\n",
        "    model.train()\n",
        "    # allocation\n",
        "    total_loss = 0  # accumulated loss\n",
        "    n_entries = 0   # accumulated number of data points\n",
        "    # progress bar def\n",
        "    train_pbar = tqdm(dataloader, desc=\"Training Epoch {epoch:2d}\".format(epoch=epoch), leave=True)\n",
        "    # training loop\n",
        "    for traces, diagnoses in train_pbar:\n",
        "        # data to device (CPU or GPU if available)\n",
        "        traces, diagnoses = traces.to(device), diagnoses.to(device)\n",
        "\n",
        "        \"\"\"\n",
        "        TASK: Insert your code here. This task can be done in 5 lines of code.\n",
        "        \"\"\"\n",
        "        optimizer.zero_grad() # Clear gradient\n",
        "        outputs = model(traces) # Forward propagation Calculate output\n",
        "        loss = loss_function(outputs, diagnoses) # Calculate loss\n",
        "        loss.backward() # Backward propagation Calculate gradient\n",
        "        optimizer.step() # Update model parameters\n",
        "\n",
        "        # Update accumulated values\n",
        "        total_loss += loss.detach().cpu().numpy()\n",
        "        n_entries += len(traces)\n",
        "\n",
        "        # Update progress bar\n",
        "        train_pbar.set_postfix({'loss': total_loss / n_entries})\n",
        "    train_pbar.close()\n",
        "    return total_loss / n_entries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQnstiLiWB1"
      },
      "source": [
        "---\n",
        "## Eval function\n",
        "\n",
        "The `eval(...)` function is similar to the `train(...)` function but is used to evaluate the model on validation data without adapting the model parameters. You can prohibit computing gradients by using a `with torch.no_grad():` statement.\n",
        "\n",
        "Currenlty only the loss is logged here. Additionally you have to collect all your predictions and the true values in order to compute more metrics such as AUROC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22G-f_ooWunl"
      },
      "source": [
        "### Coding Task 4: Fill evaluation loop\n",
        "Fill the code cell below such we obtain model predictions to evaluate the validation loss and collect the predictoin in order to compute other validation metrics in the `main(...)` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xWOvM9e5ijqk"
      },
      "outputs": [],
      "source": [
        "def eval_loop(epoch, dataloader, model, loss_function, device):\n",
        "    # model to evaluation mode (important to correctly handle dropout or batchnorm layers)\n",
        "    model.eval()\n",
        "    # allocation\n",
        "    total_loss = 0  # accumulated loss\n",
        "    n_entries = 0   # accumulated number of data points\n",
        "    valid_probs = []  # accumulated predicted probabilities\n",
        "    valid_true = [] # accumulated true labels\n",
        "\n",
        "    # progress bar def\n",
        "    eval_pbar = tqdm(dataloader, desc=\"Evaluation Epoch {epoch:2d}\".format(epoch=epoch), leave=True)\n",
        "    # evaluation loop\n",
        "    for traces_cpu, diagnoses_cpu in eval_pbar:\n",
        "        # data to device (CPU or GPU if available)\n",
        "        traces, diagnoses = traces_cpu.to(device), diagnoses_cpu.to(device)\n",
        "\n",
        "        \"\"\"\n",
        "        TASK: Insert your code here. This task can be done in 6 lines of code.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "          outputs = model(traces)\n",
        "          loss = loss_function(outputs, diagnoses)\n",
        "\n",
        "          probs = torch.sigmoid(outputs)  # change to probs\n",
        "          valid_probs.append(probs.cpu().numpy())\n",
        "          valid_true.append(diagnoses.cpu().numpy())\n",
        "\n",
        "\n",
        "        # Update accumulated values\n",
        "        total_loss += loss.detach().cpu().numpy()\n",
        "        n_entries += len(traces)\n",
        "\n",
        "        # Update progress bar\n",
        "        eval_pbar.set_postfix({'loss': total_loss / n_entries})\n",
        "    eval_pbar.close()\n",
        "\n",
        "    valid_probs = np.concatenate(valid_probs, axis=0).squeeze() # The shape of the result is (N, 1)\n",
        "    valid_true = np.concatenate(valid_true, axis=0).squeeze() # The shape of the result is (N,)\n",
        "\n",
        "    return total_loss / n_entries, valid_probs, valid_true #np.vstack(valid_probs), np.vstack(valid_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtBEPHo7jEZP"
      },
      "source": [
        "---\n",
        "## Run Training\n",
        "\n",
        "In the code cell below there are some initial (non-optimal!) training hyperparameters. Further, we combine everything from above into training code. That means that we build the dataloaders, define the model/loss/optimizer and then train/validate the model over multiple epochs. Here, we save the model with the lowest validation loss as the best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Eh5IsvQWy0y"
      },
      "source": [
        "### Coding Task 5: Combine everything to train/validate the model\n",
        "\n",
        "The following tasks are necessary in the code below\n",
        "- split the data into training and validation data\n",
        "- define the loss function\n",
        "- decide and implement validation metric(s) to evaluate and compare the model on\n",
        "\n",
        "Optional task:\n",
        "- include learning rate scheduler\n",
        "- take specific care about possible data inbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOz48rnxdGfp"
      },
      "source": [
        "### Coding Task 6: Run your model and adapt hyperparameters\n",
        "\n",
        "After you combined everything in task 5, now you run the code to evaluate the model. Based on the resulting validation metrics you tune\n",
        "- the training hyperparameters\n",
        "- the model architecture\n",
        "- the model hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjGcqXpOdSbA"
      },
      "source": [
        "### Explanation Task 3: Hyperparameter\n",
        "Please explain and motivate in short sentences or bullet points the final choice of hyperparamer and how you developed them.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "**<font color='red'>Your explanation here:</font>**\n",
        "Hyperparameter selection and motivation for choosing the values:\n",
        "- Learning rate:\n",
        "  - Determines the magnitude of parameter updates.\n",
        "  - To large -> raining loss fluctuates, difficult to converge; may cause gradient explosion in deep networks, resulting in NaN or overflow.\n",
        "  - Too small -> convergence is very slow, may get stuck in high loss regions, low training efficiency, and cannot reach low loss or high accuracy.\n",
        "\n",
        "- Weight decay:\n",
        "  - Controls the magnitude of model weights.\n",
        "  - Larger weight decay reduces weight size, preventing overfitting, but too large may cause underfitting (over-regularization).\n",
        "  - Chosen to balance between preventing overfitting and maintaining model capacity.\n",
        "\n",
        "- Num_epochs:\n",
        "  - Determines how long the model is trained. The choice is based on observing training and validation metrics:\n",
        "  - (1)Training loss and validation metrics (F1, AUROC) should stabilize\n",
        "  - (2)Early stopping can be used to avoid overfitting and automatically select optimal epoch.\n",
        "  - (3)Too few epochs → underfitting; too many epochs → overfitting and wasted computation.\n",
        "- Batch size:\n",
        "  - Balances gradient estimation stability and memory usage.\n",
        "  - Smaller batch size → noisier gradients, more fluctuation during training.\n",
        "  - Larger batch size → smoother gradients and stable training, but requires more memory to store inputs, gradients, and intermediate activations; may cause out-of-memory errors if too large.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3VNfdwn8IqQQ"
      },
      "outputs": [],
      "source": [
        "# set seed\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# choose variables\n",
        "\"\"\"\n",
        "TASK: Adapt the following hyperparameters if necessary\n",
        "\"\"\"\n",
        "learning_rate = 5e-4 #1e-3 #1e-3 #5e-4 #2e-4 #from 5e-4 to 2e-4\n",
        "weight_decay = 1e-1 #1e-1 #1e-2 #1e-1 # 1e-1 too big, change to 1e-4\n",
        "num_epochs = 15 #40 #25\n",
        "batch_size = 32 #64 #32\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CQvRyaQcyvcM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tqdm.write(\"Use device: {device:}\\n\".format(device=device))\n",
        "\n",
        "# =============== Build data loaders ======================================#\n",
        "tqdm.write(\"Building data loaders...\")\n",
        "\n",
        "path_to_h5_train, path_to_csv_train, path_to_records = 'codesubset/train.h5', 'codesubset/train.csv', 'codesubset/train/RECORDS.txt'\n",
        "# load traces\n",
        "traces = torch.tensor(h5py.File(path_to_h5_train, 'r')['tracings'][()], dtype=torch.float32)\n",
        "# load labels\n",
        "ids_traces = [int(x.split('TNMG')[1]) for x in list(pd.read_csv(path_to_records, header=None)[0])] # Get order of ids in traces\n",
        "df = pd.read_csv(path_to_csv_train)\n",
        "df.set_index('id_exam', inplace=True)\n",
        "df = df.reindex(ids_traces) # make sure the order is the same\n",
        "labels = torch.tensor(np.array(df['AF']), dtype=torch.float32).reshape(-1,1)\n",
        "# load dataset\n",
        "dataset = TensorDataset(traces, labels)\n",
        "len_dataset = len(dataset)\n",
        "n_classes = len(torch.unique(labels))\n",
        "# split data\n",
        "\"\"\"\n",
        "TASK: Split the dataset in train and validation; Insert your code here.\n",
        "This can be done in <=4 line of code\n",
        "\"\"\"\n",
        "train_size = int(0.8 * len_dataset)\n",
        "valid_size = len_dataset - train_size\n",
        "dataset_train, dataset_valid = random_split(dataset, [train_size, valid_size])\n",
        "# build data loaders\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
        "tqdm.write(\"Done!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "a4_EWu-0_GXl",
        "outputId": "e8a300a4-09e5-4972-ace1-006a37be4976"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "My Design: Data Argumentation\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class ECGDatasetWithAug(Dataset):\n",
        "    def __init__(self, traces, labels, augment=False):\n",
        "        \"\"\"\n",
        "        traces: Tensor, shape [N, Time, Channels] or [N, Channels, Time] ajust according to your data structure\n",
        "        labels: Tensor, shape [N, 1]\n",
        "        augment: bool, whether to perform data argumentation \n",
        "        \"\"\"\n",
        "        self.traces = traces\n",
        "        self.labels = labels\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.traces)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.traces[idx].clone()  # clone prevent modifying original data\n",
        "        y = self.labels[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            x = self.augment_signal(x)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def augment_signal(self, x):\n",
        "        # Assume that the shape of x is [Channels, Time] or [Time, Channels]，it is written as [Channels, Time] here. Please note that this corresponds to your actual data.\n",
        "        # 1. Random Gaussian noice\n",
        "        noise_level = 0.01\n",
        "        noise = torch.randn_like(x) * noise_level\n",
        "        x = x + noise\n",
        "\n",
        "        # 2. Random time shift (cyclic signal shift)\n",
        "        max_shift = 10  # Maximum time step shift\n",
        "        shift = random.randint(-max_shift, max_shift)\n",
        "        if shift != 0:\n",
        "            x = torch.roll(x, shifts=shift, dims=-1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tqdm.write(\"Use device: {device:}\\n\".format(device=device))\n",
        "\n",
        "# ================================ Build data loaders ======================================#\n",
        "tqdm.write(\"Building data loaders...\")\n",
        "\n",
        "path_to_h5_train, path_to_csv_train, path_to_records = 'codesubset/train.h5', 'codesubset/train.csv', 'codesubset/train/RECORDS.txt'\n",
        "# load traces\n",
        "traces = torch.tensor(h5py.File(path_to_h5_train, 'r')['tracings'][()], dtype=torch.float32)\n",
        "print(f\"trace type:{traces.shape}\")\n",
        "# load labels\n",
        "ids_traces = [int(x.split('TNMG')[1]) for x in list(pd.read_csv(path_to_records, header=None)[0])] # Get order of ids in traces\n",
        "df = pd.read_csv(path_to_csv_train)\n",
        "df.set_index('id_exam', inplace=True)\n",
        "df = df.reindex(ids_traces) # make sure the order is the same\n",
        "labels = torch.tensor(np.array(df['AF']), dtype=torch.float32).reshape(-1,1)\n",
        "\n",
        "\n",
        "\n",
        "# load dataset\n",
        "# What is the original shape of the traces? Assume it is [N, Time, Channels].\n",
        "# The original model first transposed it, so the data should be [N, Time, Channels].\n",
        "# Here, change it to [N, Channels, Time] for convenient convolution, or adjust it according to your data structure.\n",
        "\n",
        "#traces = traces.permute(0, 2, 1)  # [N, Channels, Time]\n",
        "#print(f\"trace type after permute:{traces.shape}\")\n",
        "# Custom dataset\n",
        "full_dataset = ECGDatasetWithAug(traces, labels, augment=False)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "valid_size = len(full_dataset) - train_size\n",
        "dataset_train, dataset_valid = random_split(full_dataset, [train_size, valid_size])\n",
        "# Turn on the augmentation switch for the training set.\n",
        "dataset_train.dataset.augment = True\n",
        "dataset_valid.dataset.augment = False\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
        "tqdm.write(\"Done!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tGtFsoZBzs38"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "My design: Search for the threshold that maximizes the F1 score based on the unique values among all predicted probabilities (y_prob) as the threshold.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def find_best_threshold_by_f1_exact(y_true, y_prob):\n",
        "    # Obtain the unique values from all predicted probabilities and sort them (from smallest to largest)\n",
        "    thresholds = np.sort(np.unique(y_prob))\n",
        "\n",
        "    best_t = 0.5\n",
        "    best_f1 = -1\n",
        "    best_prec = 0\n",
        "    best_rec = 0\n",
        "\n",
        "    for t in thresholds:\n",
        "        y_bin = (y_prob >=t).astype(int)\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_bin, average='binary', zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_t = t\n",
        "            best_prec = prec\n",
        "            best_rec = rec\n",
        "    return best_t, best_f1, best_prec, best_rec\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} → {val_loss:.6f}).  Saving model...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GMOKrUn9jfca"
      },
      "outputs": [],
      "source": [
        "# =============== Define model ============================================#\n",
        "tqdm.write(\"Define model...\")\n",
        "\"\"\"\n",
        "TASK: Replace the baseline model with your model; Insert your code here\n",
        "\"\"\"\n",
        "model = ResNet1D_Improved() #ResNet1D32() #ModelBaseline() #ResNet1D_Improved() #ResNet1D()\n",
        "model.to(device=device)\n",
        "tqdm.write(\"Done!\\n\")\n",
        "\n",
        "# =============== Define loss function ====================================#\n",
        "\"\"\"\n",
        "TASK: define the loss; Insert your code here. This can be done in 1 line of code\n",
        "\"\"\"\n",
        "# Define weighted loss\n",
        "#loss_function = nn.BCEWithLogitsLoss()\n",
        "\"\"\"\n",
        "My Design\n",
        "\"\"\"\n",
        "pos_weight = torch.tensor([7/3], device=device)\n",
        "loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# =============== Define optimizer ========================================#\n",
        "tqdm.write(\"Define optimiser...\")\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "tqdm.write(\"Done!\\n\")\n",
        "\n",
        "\"\"\"\n",
        "My design\n",
        "\"\"\"\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# =============== Define lr scheduler =====================================#\n",
        "# TODO advanced students (non mandatory)\n",
        "\"\"\"\n",
        "OPTIONAL: define a learning rate scheduler; Insert your code here\n",
        "\"\"\"\n",
        "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "#lr_scheduler = StepLR(optimizer, step_size=5, gamma=0.1)#None\n",
        "\n",
        "\"\"\"\n",
        "My design\n",
        "\"\"\"\n",
        "lr_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "#lr_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "\n",
        "# Early stopping\n",
        "#early_stopping = EarlyStopping(patience=10, verbose=True, path=\"model.pth\")\n",
        "\n",
        "# =============== Train model =============================================#\n",
        "tqdm.write(\"Training...\")\n",
        "best_loss = np.Inf\n",
        "\n",
        "# allocation\n",
        "train_loss_all, valid_loss_all = [], []\n",
        "\n",
        "\"\"\"\n",
        "My Design: Add codes for calculating the best threshold based on the F1 scores\n",
        "\"\"\"\n",
        "all_y_preds = []\n",
        "all_y_trues = []\n",
        "\n",
        "# loop over epochs\n",
        "for epoch in trange(1, num_epochs + 1):\n",
        "    # training loop\n",
        "    train_loss = train_loop(epoch, train_dataloader, model, optimizer, loss_function, device)\n",
        "    # validation loop\n",
        "    valid_loss, y_pred, y_true = eval_loop(epoch, valid_dataloader, model, loss_function, device)\n",
        "\n",
        "    \"\"\"\n",
        "    My Design\n",
        "    \"\"\"\n",
        "    # Call early stopping for checking\n",
        "    # early_stopping(valid_loss, model)\n",
        "\n",
        "    #if early_stopping.early_stop:\n",
        "    #    print(\"Early stopping triggered!\")\n",
        "    #    break\n",
        "\n",
        "    all_y_preds.append(y_pred)\n",
        "    all_y_trues.append(y_true)\n",
        "\n",
        "    # collect losses\n",
        "    train_loss_all.append(train_loss)\n",
        "    valid_loss_all.append(valid_loss)\n",
        "\n",
        "    # compute validation metrics for performance evaluation\n",
        "    \"\"\"\n",
        "    TASK: compute validation metrics (e.g. AUROC); Insert your code here\n",
        "    This can be done e.g. in 5 lines of code\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    valid_auroc = roc_auc_score(y_true, y_pred)\n",
        "    tqdm.write(f\"Validation AUROC: {valid_auroc:.4f}\")\n",
        "\n",
        "    \"\"\"\n",
        "    My Designs\n",
        "    \"\"\"\n",
        "    # Calculate Validation Accuracy, F1 score, AP (the default value is 0.5)\n",
        "    from sklearn.metrics import accuracy_score, f1_score, average_precision_score\n",
        "    best_threshold = find_best_threshold_by_f1_exact(y_true, y_pred)\n",
        "    y_pred_label = (y_pred > best_threshold[0]).astype(int)\n",
        "    valid_acc = accuracy_score(y_true, y_pred_label)\n",
        "    valid_f1 = f1_score(y_true, y_pred_label)\n",
        "    valid_ap = average_precision_score(y_true, y_pred)\n",
        "\n",
        "    tqdm.write(f\"Validation Accuracy: {valid_acc:.4f}\")\n",
        "    tqdm.write(f\"Validation F1: {valid_f1:.4f}\")\n",
        "    tqdm.write(f\"Validation AP: {valid_ap:.4f}\")\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        # Save model parameters\n",
        "        torch.save({'model': model.state_dict()}, 'model.pth')\n",
        "        # Update best validation loss\n",
        "        best_loss = valid_loss\n",
        "        # statement\n",
        "        model_save_state = \"Best model -> saved\"\n",
        "    else:\n",
        "        model_save_state = \"\"\n",
        "\n",
        "    # Print message\n",
        "    tqdm.write('Epoch {epoch:2d}: \\t'\n",
        "                'Train Loss {train_loss:.6f} \\t'\n",
        "                'Valid Loss {valid_loss:.6f} \\t'\n",
        "                '{model_save}'\n",
        "                .format(epoch=epoch,\n",
        "                        train_loss=train_loss,\n",
        "                        valid_loss=valid_loss,\n",
        "                        model_save=model_save_state)\n",
        "                    )\n",
        "\n",
        "    # Update learning rate with lr-scheduler\n",
        "    if lr_scheduler:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "\"\"\"\n",
        "TASK: Here it can make sense to plot your learning curve; Insert your code here\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss_all, label='Train Loss')\n",
        "plt.plot(valid_loss_all, label='Valid Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Learning Curve')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "My design: Search for the threshold that maximizes the F1 score based on the unique values among all predicted probabilities (y_prob) as the threshold.\n",
        "\"\"\"\n",
        "best_threshold, best_f1, best_precision, best_recall = find_best_threshold_by_f1_exact(all_y_trues[-1], all_y_preds[-1])\n",
        "print(f\"Best threshold: {best_threshold:.4f}\")\n",
        "print(f\"Best F1 score: {best_f1:.4f}, Best precision: {best_precision:.4f}, Best recall: {best_recall:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NHB890WDsSW0"
      },
      "outputs": [],
      "source": [
        "auroc = roc_auc_score(all_y_trues[-1], all_y_preds[-1])\n",
        "ap = average_precision_score(all_y_trues[-1], all_y_preds[-1])\n",
        "print(f\"AUROC: {auroc:.4f}, Average Precision (AP):{ap:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JGjbkZWmaR15"
      },
      "outputs": [],
      "source": [
        "# =============== Define model ============================================#\n",
        "tqdm.write(\"Define model...\")\n",
        "model = ResNet1D_Improved()\n",
        "model.to(device=device)\n",
        "tqdm.write(\"Done!\\n\")\n",
        "\n",
        "# =============== Define loss function ====================================#\n",
        "pos_weight = torch.tensor([7/3], device=device)\n",
        "loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# =============== Define optimizer ========================================#\n",
        "tqdm.write(\"Define optimiser...\")\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "tqdm.write(\"Done!\\n\")\n",
        "\n",
        "# =============== Define lr scheduler =====================================#\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# =============== Early stopping ==========================================#\n",
        "early_stopping = EarlyStopping(patience=8, verbose=True, path=\"model.pth\")\n",
        "\n",
        "# =============== Train model =============================================#\n",
        "tqdm.write(\"Training...\")\n",
        "best_loss = np.Inf\n",
        "train_loss_all, valid_loss_all = [], []\n",
        "\n",
        "all_y_preds = []\n",
        "all_y_trues = []\n",
        "\n",
        "for epoch in trange(1, num_epochs + 1):\n",
        "    # training loop\n",
        "    train_loss = train_loop(epoch, train_dataloader, model, optimizer, loss_function, device)\n",
        "    # validation loop\n",
        "    valid_loss, y_pred, y_true = eval_loop(epoch, valid_dataloader, model, loss_function, device)\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping(valid_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "    # Append predictions and labels\n",
        "    all_y_preds.append(y_pred)\n",
        "    all_y_trues.append(y_true)\n",
        "\n",
        "    # Collect losses\n",
        "    train_loss_all.append(train_loss)\n",
        "    valid_loss_all.append(valid_loss)\n",
        "\n",
        "    # Compute validation metrics\n",
        "    from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, average_precision_score\n",
        "    valid_auroc = roc_auc_score(y_true, y_pred)\n",
        "    best_threshold = find_best_threshold_by_f1_exact(y_true, y_pred)\n",
        "    y_pred_label = (y_pred > best_threshold[0]).astype(int)\n",
        "    valid_acc = accuracy_score(y_true, y_pred_label)\n",
        "    valid_f1 = f1_score(y_true, y_pred_label)\n",
        "    valid_ap = average_precision_score(y_true, y_pred)\n",
        "\n",
        "    tqdm.write(f\"Validation AUROC: {valid_auroc:.4f}\")\n",
        "    tqdm.write(f\"Validation Accuracy: {valid_acc:.4f}\")\n",
        "    tqdm.write(f\"Validation F1: {valid_f1:.4f}\")\n",
        "    tqdm.write(f\"Validation AP: {valid_ap:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if valid_loss < best_loss:\n",
        "        torch.save({'model': model.state_dict()}, 'model_Reduce.pth')\n",
        "        best_loss = valid_loss\n",
        "        model_save_state = \"Best model -> saved\"\n",
        "    else:\n",
        "        model_save_state = \"\"\n",
        "\n",
        "    tqdm.write('Epoch {epoch:2d}: \\t'\n",
        "                'Train Loss {train_loss:.6f} \\t'\n",
        "                'Valid Loss {valid_loss:.6f} \\t'\n",
        "                '{model_save}'\n",
        "                .format(epoch=epoch,\n",
        "                        train_loss=train_loss,\n",
        "                        valid_loss=valid_loss,\n",
        "                        model_save=model_save_state)\n",
        "                )\n",
        "\n",
        "    # Update learning rate based on validation loss\n",
        "    if lr_scheduler:\n",
        "        lr_scheduler.step(valid_f1)\n",
        "\n",
        "# =============== Plot learning curve =====================================#\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss_all, label='Train Loss')\n",
        "plt.plot(valid_loss_all, label='Valid Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Learning Curve')\n",
        "plt.show()\n",
        "\n",
        "# =============== Best threshold & metrics =================================#\n",
        "best_threshold, best_f1, best_precision, best_recall = find_best_threshold_by_f1_exact(\n",
        "    all_y_trues[-1], all_y_preds[-1])\n",
        "print(f\"Best threshold: {best_threshold:.4f}\")\n",
        "print(f\"Best F1 score: {best_f1:.4f}, Best precision: {best_precision:.4f}, Best recall: {best_recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JpmBrr5rUVVk"
      },
      "outputs": [],
      "source": [
        "auroc = roc_auc_score(all_y_trues[-1], all_y_preds[-1])\n",
        "ap = average_precision_score(all_y_trues[-1], all_y_preds[-1])\n",
        "print(f\"AUROC: {auroc:.4f}, Average Precision (AP):{ap:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ouhd4vgt4jlS"
      },
      "source": [
        "---\n",
        "## Model Testing\n",
        "\n",
        "Since we saved our best model, we can now load the trained model and make predictions on the test data set. We save the predictions in a csv file which will be uploaded as part of the deliverables. Note that we take a `Sigmoid()` function on the model prediction in order to obtain soft predictions (probabilities) instead of hard predictions (0s or 1s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXJRnGTpW7Qv"
      },
      "source": [
        "### Coding Task 7: Make prediction for test data\n",
        "\n",
        "Here you do not really need to code but you have to:\n",
        "- replace the baseline model with your model. If you do not use colab then change the path to the model location to load the trained model)\n",
        "- run the script. The predictions are saved in the variable `soft_pred`.\n",
        "- upload your predictions to the leaderboard online (see instruction details below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fZwmInejWGy4"
      },
      "outputs": [],
      "source": [
        "# build the dataloader once and re-use when running the cell below possibly multiple times.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# =============== Build data loaders ==========================================#\n",
        "tqdm.write(\"Building data loaders...\")\n",
        "# load data\n",
        "path_to_h5_test, path_to_csv_test = 'codesubset/test.h5', 'codesubset/test.csv'\n",
        "traces = torch.tensor(h5py.File(path_to_h5_test, 'r')['tracings'][()], dtype=torch.float32)\n",
        "dataset = TensorDataset(traces)\n",
        "len_dataset = len(dataset)\n",
        "# build data loaders\n",
        "test_dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "tqdm.write(\"Done!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0-m4CYeW_hvO"
      },
      "outputs": [],
      "source": [
        "# =============== Define model ================================================#\n",
        "tqdm.write(\"Define model...\")\n",
        "\"\"\"\n",
        "TASK: Replace the baseline model with your model; Insert your code here\n",
        "\"\"\"\n",
        "model = ResNet1D_Improved()\n",
        "\n",
        "# load stored model parameters\n",
        "ckpt = torch.load('model.pth', map_location=lambda storage, loc: storage)\n",
        "model.load_state_dict(ckpt['model'])\n",
        "# put model on device\n",
        "model.to(device=device)\n",
        "tqdm.write(\"Done!\\n\")\n",
        "\n",
        "# =============== Evaluate model ==============================================#\n",
        "model.eval()\n",
        "# allocation\n",
        "test_pred = torch.zeros(len_dataset,1)\n",
        "# progress bar def\n",
        "test_pbar = tqdm(test_dataloader, desc=\"Testing\")\n",
        "# evaluation loop\n",
        "end=0\n",
        "for traces in test_pbar:\n",
        "    # data to device\n",
        "    traces = traces[0].to(device)\n",
        "    start = end\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        model_output = model(traces)\n",
        "\n",
        "        # store output\n",
        "        end = min(start + len(model_output), test_pred.shape[0])\n",
        "        test_pred[start:end] = torch.nn.Sigmoid()(model_output).detach().cpu()\n",
        "\n",
        "test_pbar.close()\n",
        "\n",
        "# =============== Save predictions ============================================#\n",
        "soft_pred = np.stack((1-test_pred.numpy(), test_pred.numpy()),axis=1).squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMxVJxHsgGdT"
      },
      "source": [
        "To upload your predictions to the leaderboard, use the following code. There are the following steps to follow:\n",
        "1. Download the GitHub repository for the leaderboard submission system.\n",
        "2. Register your team with a **team id** and **password**. The password ensures that only your team can upload to your team id. Do only run the registration once.\n",
        "3. Upload you predictions as a new submission. There are some things to obey here:\n",
        "    - For each submission you have to attach a note for you to keep track of the submission in the leaderboard and for us to know which submission you refer to in your explanation. Choose something meaningful such as \"submission A\" or \"model B\".\n",
        "    - You can only get one prediction evaluated per day and you get the score the following day. If you do multiple submissions on the same day, the initial submission will be overwritten and thus only the final submission will be evaluated.\n",
        "    - Only a maximum of ***FIVE*** submissions will be evaluated. So make them count! (If you update an submission before it is evaluated it doesn't count)\n",
        "    - The evaluation score is published with you team_id and note at http://hyperion.it.uu.se:5050/leaderboard\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-BPWcP20RHwu"
      },
      "outputs": [],
      "source": [
        "# 1. Download repository for leaderboard submission system\n",
        "if not exists('leaderboard'):\n",
        "    !git clone https://gist.github.com/3ff6c4c867331c0bf334301842d753c7.git leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DRaCnCLsgHPg"
      },
      "outputs": [],
      "source": [
        "# 2. Registration of your team\n",
        "host = \"http://hyperion.it.uu.se:5050/\"\n",
        "runfile(\"leaderboard/leaderboard_helpers.py\")\n",
        "\n",
        "\"\"\"\n",
        "TASK: Decide for a team_id (max 20 chars) and password.\n",
        "Do not change this after you have registered your team\n",
        "\"\"\"\n",
        "team_id = 'YangyangWen_Group25' #Fill in a string\n",
        "password = 'yyw_group25' #Fill in a string\n",
        "\n",
        "# run the registration\n",
        "r = register_team(team_id, password)\n",
        "if (r.status_code == 201):\n",
        "    print(\"Team registered successfully! Good luck\")\n",
        "elif not (r.status_code == 200):\n",
        "    raise Exception(\"You can not change your password once created. If you need help, please contact the teachers\")\n",
        "print(r.status_code)\n",
        "print(r.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I03QEBiegF7_"
      },
      "outputs": [],
      "source": [
        "# 3. Upload the prediction as submission\n",
        "\n",
        "# Write a note about the training procedure so you can identify it in the leaderboard. e.g. 5 epochs, or First  (Max 20 characters)\n",
        "\"\"\"\n",
        "TASK: Add a note for you submission\n",
        "\"\"\"\n",
        "note = 'Submission 3' #Fill in a string\n",
        "\n",
        "# Submit the predictions to the leaderboard. Note, this also saves your submissions in your colab folder\n",
        "r = submit(team_id, password, soft_pred.tolist(), note)\n",
        "if r.status_code == 201:\n",
        "    print(\"Submission successful!\")\n",
        "elif r.status_code == 200:\n",
        "    print(\"Submission updated!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ndDdQGJrTd24"
      },
      "outputs": [],
      "source": [
        "print(r.status_code)\n",
        "print(r.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTKe9X-zTt7f"
      },
      "source": [
        "### Explanation Task 4: Submissions\n",
        "One of the grading criteria are three submissions to the leaderboard. List the three main submissions in the table below and explain the main changes in your code for each submission.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "**<font color='red'>Your explanation here:</font>**\n",
        "\n",
        "Submission 1:\n",
        "  - After testing multiple CNN architectures (ModelBaseline, Model, LightCNN, ResNet1D, and ResNet1D_Improved), I selected ResNet1D_Improved as it achieved the best results. The architecture includes residual blocks with larger kernels and dropout layers:\n",
        "    - Layer1: kernel=7, dropout=0.1\n",
        "    - Layer2: kernel=5, dropout=0.2, stride=2\n",
        "    - Layer3: kernel=5, dropout=0.3, stride=2\n",
        "    - Layer4: kernel=3, dropout=0.3, stride=2 (new additional layer)\n",
        "\n",
        "Submission 2:\n",
        "- Identified class imbalance in the dataset (7,000 positive vs. 3,000 negative samples). Adjusted the loss function with class weights to address imbalance:\n",
        "```python\n",
        "pos_weight = torch.tensor([7/3], device=device)\n",
        "loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "\n",
        "Submission 3:\n",
        "- Experimented with different optimizers (Adam, AdamW) and learning rate schedulers (StepLR, CosineAnnealingLR, CosineAnnealingWarmRestarts). Found that AdamW with CosineAnnealingLR provided slight performance improvements.\n",
        "\n",
        "Your team id: **<font color='red'>YangyangWen_Group25</font>**\n",
        "\n",
        "| Submission note | Accuracy | F1 | AUC | AP | Submission description |\n",
        "| --------------- | -------- | -- | --  | -- | ---------------------- |\n",
        "|ImprovedResnet1D              | 0.910        | 0.905  | 0.972   | 0.965  | Submission 1                   |\n",
        "|Imbalanced samples, changed loss function weights              | 0.931        | 0.931  | 0.978   | 0.977  | Submission 2                   |\n",
        "|Optimizers changes             | 0.938        | 0.938  | 0.980   | 0.978  | Submission 3                   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw-GKZ4fVDhD"
      },
      "source": [
        "### Explanation Task 5: Reflection on Metrics\n",
        "Your were asked to reach a certain value in AUC and AP while maximising F1 for the leaderboard position. Explain in bullet points what aspect each of the metrics covers and why it is important not to just focus on one metric. What can happen if you only focus on AUC for example?\n",
        "\n",
        "<br />\n",
        "\n",
        "**<font color='red'>Your explanation here:</font>**\n",
        "\n",
        "ROC curve & AUROC\n",
        "- ROC plots False Positive Rate (x-axis) vs True Positive Rate (y-axis) across different thresholds.\n",
        "- AUROC is the area under this curve, representing the probability that a randomly chosen positive is ranked higher than a negative.\n",
        "- Higher AUROC means better separation between positive and negative classes.\n",
        "\n",
        "Average Precision (AP)\n",
        "- Computed from the Precision–Recall curve (Precision on y-axis, Recall on x-axis).\n",
        "- Calculated by sorting predictions by confidence score, evaluating Precision and Recall at multiple thresholds, and taking a weighted average.\n",
        "- More sensitive to class imbalance than AUROC — low AP indicates poor positive-class performance even when AUROC is high.\n",
        "\n",
        "F1-score\n",
        "- Harmonic mean of Precision and Recall, reflecting the trade-off between retrieving positives and avoiding false positives.\n",
        "- Particularly relevant for imbalanced datasets where accuracy is misleading.\n",
        "\n",
        "Why not focus on one metric only?\n",
        "- Optimising only AUROC may yield high ranking ability but very low Precision if the threshold leads to over-predicting positives.\n",
        "- Balanced evaluation across AUROC, AP, and F1 ensures both ranking quality and usable Precision–Recall trade-offs.\n",
        "\n",
        "In summary, AUC focuses on overall ranking ability and is not affected by the ratio of positive and negative classes. AP is sensitive to the ratio of positive classes and focuses more on the capture effect of positive classes. F1 is not misled by overall accuracy, but rather exposes the “false high” of accuracy in unbalanced data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95dtdljFjYn3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
